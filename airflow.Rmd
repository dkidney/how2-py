---
title: airflow
editor_options: 
  chunk_output_type: console
---

```{r knitr_config, code=readLines("_knitr_config.R"), echo=FALSE}
```

*********************************************************************

> A platform to programmatically author, schedule and monitor workflows

<!-- ********************************************************************* -->

<!-- ## faqs -->

*********************************************************************

## links

* https://airflow.apache.org/  
* https://airflow.apache.org/docs/stable/  
* https://airflow-tutorial.readthedocs.io/en/latest/index.html  

*********************************************************************

## setup

setup a virtual environment:

```sh
env_name=airflow
conda create -y --name ${env_name} python=3.7.6
conda activate ${env_name}
```

install and initialise airflow:

```sh
# set an alternative installation directory if you don't want the default
export AIRFLOW_HOME=~/airflow  # ~/airflow is the default

# install airflow and dependencies
pip install -U apache-airflow  # installs to ${AIRFLOW_HOME}
pip install -U boto3
pip install -U cryptography # recommended

# create and initialize the airflow sqlite database
airflow initdb
```

check version:

```sh
airflow version
```

```{python}
import airflow
airflow.__version__
```

### folder structure

Dags should be placed in `${AIRFLOW_HOME}/dags/`

```sh
tree ${AIRFLOW_HOME} -L 1
#├── airflow.cfg
#├── airflow.db
#├── dags
#├── logs
#└── unittests.cfg
tree ${AIRFLOW_HOME}/dags -L 1
#├── __pycache__
#└── test_dag.py
```

### fernet key

* https://airflow.readthedocs.io/en/stable/howto/secure-connections.html

```sh
# make sure your airflow.cfg file has a fernet_key
grep fernet_key ~/airflow/airflow.cfg
```

If you don't have a `fernet_key` then generate one in a python session

```python
from cryptography.fernet import Fernet
fernet_key= Fernet.generate_key()
print(fernet_key.decode()) # keep it in a safe place!
```

Then either edit `${AIRFLOW_HOME}/airflow.cfg` manually to permanently add the key to your airflow setup, or set it as an environmental variable:

```sh
export AIRFLOW__CORE__FERNET_KEY=your_fernet_key
```

*********************************************************************

## quick start

Open two separate terminals:

1. start the web server

```sh
conda activate airflow
airflow webserver -p 8080 # default port is 8080
```

2. start the scheduler

```sh
conda activate airflow
airflow scheduler
```

visit http://localhost:8080/admin/ and enable the example dag in the home page

The web interface should show real-time updates to your dags in `${AIRFLOW_HOME}/dags/` as long as the scheduler is running.


*********************************************************************

## basics

### terms

* **task/operator** - a defined unit of work  
* **task instance** - an individual run of a single task  
* **dag** - directed acyclic graph - a set of tasks with explicit execution order  
* **dag run** - individual execution/run of a DAG  

### components

* **web server** - a gui where you can track job status and read logs from a remote file store  
* **scheduler** - responsible for scheduling jobs  
* **executor** - the mechanism that gets the tasks done  
* **metadata database** - powers how the other components interact and stores the Airflow states  


*********************************************************************

## operators

### PythonOperator

```python
def print_context(ds, **kwargs):
    pprint(kwargs)
    print(ds)
    return 'Whatever you return gets printed in the logs'

run_this = PythonOperator(
    task_id='print_the_context',
    provide_context=True,
    python_callable=print_context,
    dag=dag,
)
```

### BashOperator

```python
t1 = BashOperator(task_id='print_date',
    bash_command='date,
    dag=dag) 
```

### EmailOperator

### SimpleHttpOperator

### MySqlOperator


*********************************************************************

## example DAG

```python
# 1. imports
import datetime as dt
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import json
import boto3
from airflow.utils.dates import days_ago

# 2. default arguments
default_args = {
    'owner': 'me',
    'start_date': days_ago(2),
    'retries': 1,
    'retry_delay': dt.timedelta(minutes=1),
}

# 3. define tasks
def save_to_s3():
    auth_aws_datascience = BaseHook.get_connection('aws_datascience').extra_dejson
    aws_key = auth_aws_datascience['aws_access_key_id']
    aws_secret = auth_aws_datascience['aws_secret_access_key']
    blank = {}
    tdate = str(dt.date.today())
    with open('empty.json','w') as file:
        json.dump(blank,file)
    s3 = boto3.client('s3', aws_access_key_id=aws_key,
                      aws_secret_access_key=aws_secret)
    file_name = '{}_dk.json'.format(tdate)
    s3.upload_file('empty.json', 'ds-datamart', 'model_dar_test/'+file_name)

# 4. instantiate a DAG
with DAG('save_s3',
         default_args=default_args,
         schedule_interval='@daily',
         ) as dag:
    print_save = BashOperator(task_id='print_save',
                               bash_command='echo "Saving File to S3"')
    save_file = PythonOperator(task_id='save_s3',
                                 python_callable=save_to_s3)

# 5. specify operator order
print_save >> save_file
```

*********************************************************************

## aws 

* need to provide aws access key and secret key, so that they can be accessed using `BaseHook`  
* go to airflow web interface, then click admin, connections  
* in connections, add new name (?)  
* in the extra textbox, add a dictionary containing region_name, aws_access_key_id and aws_secret_access_key  

<!-- ********************************************************************* -->

<!-- ## appendix: dir -->

<!-- ```{python} -->
<!-- from pprint import pprint -->
<!-- import todo -->
<!-- pprint(dir(todo)) -->
<!-- ``` -->