---
title: pyspark
editor_options: 
  chunk_output_type: console
---

```{r knitr_config, code=readLines("_knitr_config.R"), echo=FALSE}
```

*********************************************************************

> An interface for Apache Spark in Python

<!-- ********************************************************************* -->

<!-- ## faqs -->

*********************************************************************

## links

* https://spark.apache.org/docs/latest/api/python/  
* https://realpython.com/pyspark-intro/  

*********************************************************************

## setup

```sh
pip install pyspark
```

<!-- ```{python} -->
<!-- import todo -->
<!-- todo.__version__ -->
<!-- ``` -->

*********************************************************************

## basics

The * tells Spark to create as many worker threads as logical cores on your machine.

```python
import pyspark
sc = pyspark.SparkContext('local[*]')
```

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
```

*********************************************************************

## RDDs

Resilient Distributed Datasets.

### textFile

```python
rdd = sc.textFile('path')
```

### parallelize

Transforms some data structures (e.g. lists, tuples) into RDDs.

```python
iterator = range(10000)
rdd = sc.parallelize(iterator, 2)
even_numbers = rdd.filter(lambda x: x % 2 == 0)
even_numbers.take(5)
```





*********************************************************************

## spark data frame

View schema
```python
df.printSchema()
```

View top row
```python
df.show(1)
df.show(1, vertical=True)
```

View columns
```python
df.columns
```

Count rows
```python
df.count()
```

Summarise
```python
df.describe().show()
```

Select columns
```python
df.col
df.select('col')
df.select(df.col)
df.select(col("col1"), col("col2"), col("col3"))
```

Rename columns
```python
df = df.withColumnRenamed('old_name', 'new_name')
```

Copy columns
```python
df = df.withColumn('new_col', df['existing_col'])
```

Filter rows
```python
df.filter(df.a == 1)
```

Count distinct
```python
count_distinct_col = df.select("col").distinct().count()
```

Join tables
```python
df3 = df1.join(df2, on='col', how='inner')
df3 = df1.join(df2, on='col', how='left_anti')
```

Create temp view for sql querying
```python
df.createOrReplaceTempView('table_name')
```

Collect into local memory
```python
df.collect()
df.take(1)
```

Write table
```python
df.write.mode('overwrite').partitionBy('partition_col').saveAsTable('table_name')
```

From/to parquet
```python
spark.read.csv('asdf.parquet', header=True)
df.write.parquet('asdf.parquet', header=True, mode="overwrite")
```

From/to csv
```python
spark.read.csv('asdf.csv', header=True)
df.write.csv('asdf.csv', header=True, mode="overwrite")
```

*********************************************************************

## Pandas 

From/to pandas df
```python
df = spark.createDataFrame(pandas_df)
pandas_df = df.select("*").toPandas()
```

### pandas_udf

* https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html  
* https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf  

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -> pd.Series:
    return series + 1

df.select(pandas_plus_one(df.a)).show()
```

### mapInPandas

*********************************************************************

## sql

Make a temp view table from a spark df
```python
df.createOrReplaceTempView("my_temp_view_table")
```

Perform queries
```python
spark.sql("select count(*) from my_temp_view_table").show()
df2 = spark.sql("select * from my_temp_view_table limit 10")
```

Use pandas UDFs
```python
@pandas_udf("integer")
def add_one(s: pd.Series) -> pd.Series:
    return s + 1

spark.udf.register("add_one", add_one)
spark.sql("SELECT add_one(v1) FROM tableA").show()
```

*********************************************************************

## misc

Invoke garbage collector
```python
spark.sparkContext._jvm.System.gc()
```

<!-- ********************************************************************* -->

<!-- ## appendix: dir -->

<!-- ```{python} -->
<!-- from pprint import pprint -->
<!-- import todo -->
<!-- pprint(dir(todo)) -->
<!-- ``` -->